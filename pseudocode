#user provides start url, keywords, archive directory(optional),

"""
Example dictionary

Neurips.shelf = 
{
    year: {
        title: {
            keywords: [w1, w2, ...],
            url: "https://...",
            user_defined_tags: [t1, t2, t3], #overwrites the keyword query (paper includes rl but not rl paper)
        }
    }
}

Alternative

Neurips.shelf = 
{
    year: {
        num_papers: 500, # probs dont need this as can take len(papers.keys())
        papers: {
            title: {
                keywords: [w1, w2, ...],
                url: "https://...",
                user_defined_tags: [t1, t2, t3], #overwrites the keyword query (paper includes rl but not rl paper)
            }
        }
    }
}


Alternative 2

shelf = 
{
    conference: {
        year: {
            num_papers: 500,
            papers: {
                title: {
                    keywords: [w1, w2, ...],
                    url: "https://...",
                    user_defined_tags: [t1, t2, t3], #overwrites the keyword query (paper includes rl but not rl paper)
                }
            }
        }
    }
}

"""

from pydoc import pager


for each url:
    if url ends with "pdf":
        get link anchor text as the name.
        check name against json file
        if name in json file:
            if keywords in json file:
                if file not in archive directory:
                    download file and put in archive directory

        if name not in json file:
            calculate keywords
            add to json


# Write function that determines how many papers there are to download
# use this function with tqdm to give indication of how long left


neurips:

    change the xpath specifier to find only the links for each paper page 
    